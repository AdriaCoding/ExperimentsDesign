---
title: "SMDE: Second Assignment"
author: "Adri√† Lisa"
date: "2023-12-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## 1. Defining our own RNG
We define a function to define $n$ pseudo-random numbers between $[0,1)$ with a given *seed*, for reproducibility.
A congruential generator was used, with the following hyperparameters:

* Module $m = 2^32$.
* Multiplicative factor $a = 4 \cdot 777 + 1$.
* Additive factor $c = 50033$.

A list of RN is generated via the recurrence $Z_i = a Z_{i-1} + c \,\, (mod \,m)$, where $Z_0$ is the value of the *seed*, set by default to the prime number $26107$.
```{r}
My_RNG <- function (n, seed=26107) {
  #Congruent mixt sequence of numbers (best if c and Z0 are prime)
  m = 2^32; a = 4*777+1; c = 50033 ;
  R = integer(n); R[1] <- (a*seed + c)%%m 
  for (i in 1:(n-1)){
    R[i+1] <- (a*R[i] + c)%%m
  }
  R = R / m # in [0, 1)
  return(R)
}
```
Our teacher (Pau Fonseca) explained in class that although it is not mathematically proven, it has been empirically observed that choosing the seed to be a prime number often leads to better results.

As part of our validation process, we tested our RNG with different empirical tests from the ```{r} randtests``` library, obtaining a reasonably high confidence in the hypothesis of "randomness".

```{r}
R = My_RNG(10000)
library(randtests)
bartels.rank.test(R)
turning.point.test(R)
cox.stuart.test(R)
difference.sign.test(R)
rank.test(R)
```
## 2. Simulating our data
We created a table of numeric data, using R's standard RNG, and save it in the file "rngdata.csv".

The first 5 factors in our dataset are just random numbers with a uniform distribution, $F_1,F_2, F_3, F_4, F_5 \sim U(0,10)$.

```{r}
# Set seed for reproducibility
set.seed(123)

# Number of individuals and factors
num_individuals <- 2000
num_factors <- 10

# Create an empty data frame
data <- data.frame(matrix(NA, nrow = num_individuals, ncol = num_factors + 1))
colnames(data) <- c(paste("Factor", 1:num_factors, sep = ""), "Answer")

# Define distributions for Factor 1 to 5
for (i in 1:5) {
  data[, i] <- runif(num_individuals, min=0, max = 10)
}

```
The last 5 factors are linear combinations of the previous ones, namely:
 * $F_6 = 6F_1 + F_3$
 * $F_7 = F_1 + F_2 + F_3 + F_4 + F_5$
 * $F_8 = F_1 - F_5$
 * $F_9 = 3F_3 - 2 F_4$
 * $F_{10} = -7F_2 - 5 F_4$
```{r}

# Define linear functions for Factor 6 to 10

data$Factor6 <- 6*data$Factor1 + data$Factor3
data$Factor7 <- data$Factor1 + data$Factor2 + data$Factor3 + data$Factor4 + data$Factor5
data$Factor8 <- data$Factor1 - data$Factor5
data$Factor9 <- 3*data$Factor3 - 2*data$Factor4
data$Factor10 <- -7*data$Factor2 - 5*data$Factor4
```
Finally, an Answer variable is defined as the sum of factors $F_4, F_5, F_6$ + a random noise $Z \sim N(0,1)$.
```{r}
# Answer variable is just the sum of all factors + noise
data$Answer <- rowSums(data[,4:6]) + rnorm(num_individuals)
write.csv(data, "rngdata.csv", row.names = FALSE)
```
With this definitions, one can see that in actuality, we have a formula for the Answer variable using only independent Factors:
$$
Answer = 6*F_1 + F_3 + F_4 + F_5 + Z
$$
And by the fact that $F_2$ does not appear in this formula, we can expect to find that it will not be a useful factor when we try to predict the Answer variable.


## 3. Obtain an expression to generate new data
In this section we forget our knowledge about the dataset, and wish to develop a linear model for the Answer variable.

### Correlation Analysis
First, we will look at the correlation between the Answer variable and our Factors.
```{r, echo=FALSE}
# Read data from a CSV file, in case we want to use the code for other datasets. 
data <- read.csv("rngdata.csv")

# Correlation matrix
cor_matrix <- cor(data)
print(cor_matrix[,11])

```
Factors $F1, F6, F7, F8$ are the most correlated with $Answer$.

### SLR Analysis
Then, we explored simple linear regression (SLR) models for every Factor:
$$
 Y = \beta_0 + \beta_1 * F_i \quad \quad \forall i\in[1, dots,  10] 
$$
From all the information we can get from a linear model, we are only interested in two things:
 * The $R^2$ values, which account for the ability of the linear model to explain the variability of the data.
 * The $p$-value of the significance for $\beta_i$, that is, $Prob(\beta_1 \neq 0)$.


```{r}
lm_list <- vector("list", length = 10)

# Simple linear regression for each factor vs. Answer
for (i in 1:10) {
  lm_list[[i]]<- lm(Answer ~ data[, i], data = data)
  model_summary <- summary(lm_list[[i]])
  print(paste("Factor", i, "--> R^2 = ", model_summary$r.squared, 
              ", t test p-value =", model_summary$coefficients[2,4]))
}
```
Factors $F_1, F_6, F_7, F_8$ are again shown to be the most relevant, with factor $F_6$ having a whopping $R^2 = 0.94$ in its simple LRM. 
Notably, the SLR models of Factors $F_2, F_9$ are very poor. For instance, we can not accept that the coefficent for $F_9$ is signicant at all, as the $p-value$ is close to $0.2$.

### MLR model
We now see what happens when we build a multiple linear regression (MLR), with all the factors:
$$
Y = \beta_0 + \sum_{i=1}^{10} \beta_i * F_i 
$$
```{r}
# Initial Multiple linear regression model
first_mlr_model <- lm(Answer ~ ., data = data)
summary(first_mlr_model)
```
Factors $F_6, \dots, F_{10}$ are automatically discarded. This is not surprising as we now that they are actually linearly dependant on the first Factors $F_1, \dots, F_5$. 

We can see that factor $F_2$ has a very small and insignificant coefficient $\beta_2 \simeq 0$, so we may just as well exclude it from the model.

To be completely sure about the correctness of removing $F_2$, we can also conduct a partial F-test:

```{r}
# Final Multiple linear regression model
mlr_model <- lm(Answer ~ Factor1 + Factor3 + Factor4 + Factor5, data = data)

# Partial F-Test
SSE_reduced_model <- sum((mlr_model$residuals)^2)
SSE_full_model <- sum((first_mlr_model$residuals)^2)
removed_factors <- mlr_model$df.residual - first_mlr_model$df.residual # 1
F_statistic <- (SSE_reduced_model - SSE_full_model)/removed_factors
F_statistic <- F_statistic * first_mlr_model$df.residual/SSE_reduced_model
p_value <- pf(F_statistic, removed_factors, first_mlr_model$df.residual)
if (p_value < 0.05) {
  print("We can safely remove Factor2 from the model!")
} else {
  print("Factor2 should not be removed from the model")
}
```
The final MLR model has this shape:

$$
Y = \beta_0 + \beta_1 * F_1 + \beta_2 * F_3 + \beta_3 * F_4 + \beta_4 * F_5
$$

We shall now test the assumptions of this MLR model, as part of our validation processes (see part 4 of the assignment).

##### 1-Linearity of the relationship in the data 
```{r}
# We expect to see an red line close to the horizontal at 0.
plot(mlr_model, 1)
```
##### 2- Residuals Normally distributed
```{r}
# We expect the QQ plot to follow the straight dashed line
plot(mlr_model, 2)
```

```{r}
# Shapiro-Wilks Test, we expect p-value > 0.1 (see R documentation)
shapiro.test(residuals(mlr_model))
```
##### 3-Homoscedasticity

```{r}
# We want all the residuals to have the same finite variance.
library(lmtest)
bptest(mlr_model) # We expect p-value > 0.05 (most common significance value)
```
#### 4-Independence of the errors
```{r}
# Durbin-Watson test for autocorrelation in the errors.
# We expect p-value > 0.05 to accept the independence hypothesis.
dwtest(mlr_model)
```

#### 5-Factors are uncorrelated
```{r}
# Check the Variance Inflation Factors (VIF)
# We expect all of them to be close to one.
library(car)
vif(mlr_model)
```

All the tests pass, so we conclude our MLR model is valid.

### PCA model

However, we believe that we can build a lighter model, that contains less than 4 input variables.
To do so, we now perform a principal component analysis (PCA).

```{r}
pca<-prcomp(data[, -ncol(data)])
summary(pca)
```

Since we can achive 88% explainablity for the variance by just taking the first 2 components, we will build a linear model from PC1 and PC2. 
```{r}
pca_data <- as.data.frame(cbind(pca$x[,1:2], data$Answer))
pca_model <- lm(V3 ~ ., data = pca_data)
summary(pca_model)

```
All the test statistics of this model are also very good, so we might prefere to use this one over the MLR because it is lighter. 

Even so, we will now make some tests on this model

##### 1-Linearity of the relationship in the data 
```{r}
# We expect to see an red line close to the horizontal at 0.
plot(pca_model, 1)
```

#### 2- Residuals Normally distributed
```{r}
# We expect the QQ plot to follow the straight dashed line
plot(pca_model, 2)
```


```{r}
# Shapiro-Wilks Test, we expect p-value > 0.1 (see R documentation)
shapiro.test(residuals(pca_model)) #This test actually fails
```

#### 3-Homoscedasticity

```{r}
# We want all the residuals to have the same finite variance.
library(lmtest)
bptest(pca_model) # We expect p-value > 0.05 (most common significance value)
```

#### 4-Independence of the errors
```{r}
# Durbin-Watson test for autocorrelation in the errors.
# We expect p-value > 0.05 to accept the independence hypothesis.
dwtest(pca_model)
```


#### 5-Factors are uncorrelated
```{r}
# Check the Variance Inflation Factors (VIF)
#
# This holds by definition in models from a PCA,
# so all of them will be exactly 1.
library(car)
vif(pca_model)
```
For this model, we see that one of the assumptions is not holding:
 **The residuals are not normally distributed!**
The failure of this assumption is not a sufficient condition for discarding the model. Even so, we have to be aware of it, as it usually leads to an overestimation of a model's performance statistics.

### MLR vs PCA discussion
Let us now make a thorough investigation on the residuals for both models: 

```{r}
x <- seq(min(pca_model$residuals), max(pca_model$residuals), length=40)
curve <- dnorm(x, 0, sd(pca_model$residuals))

{
hist(x = pca_model$residuals, freq=FALSE, breaks=20,
     main ="Residuals Histogram for PCA model")
lines(x = x, y= curve, col = "red")
lines(x = density(x = pca_model$residuals), col= "blue")
}
```
```{r}
x <- seq(min(mlr_model$residuals), max(mlr_model$residuals), length=40)
curve <- dnorm(x, 0, sd(mlr_model$residuals))

{
hist(x = mlr_model$residuals, freq=FALSE, breaks=20,
     main ="Residuals Histogram for MLR model")
lines(x = x, y= curve, col = "red")
lines(x = density(x = mlr_model$residuals), col= "blue")
}
```

In the above plots, we see <span style="color:blue"> a normal distribution</span> fitting the residuals vs the <span style="color:red"> an empirical distribution</span> obtained form the histogram. We see that both pairs of lines are quite similar, with the main difference being that the residuals form the PCA model are wider. 

If we look at the empirical standard deviation of the residuals, we see that the PCA model's is four times larger than MLR model's:
```{r, echo=FALSE}
print(paste("Standard deviation of errors in PCA model: ",sd(pca_model$residuals)))
print(paste("Standard deviation of errors in MLR model: ",sd(mlr_model$residuals)))
```
This makes us conclude that the PCA model is inferior to the MLR. Is is quite possible that the failure of the second assumption led to an overestimation of the performance statistics for the PCA model. Thus, in a real situation we will be choosing the MLR model.

However, if we remember the way we defined our data, we realize that the standard deviation of the MLR is so close to one because it accounts for the random noise $Z \sim N(0,1)$. The MLR is the perfect model to represent our data, and as such, it is not very interesting.

In the end, we decided to stick to the PCA model for fun. It will allow us to reach more interesting conclusions in the third part, as it uses information from all the factors, and it will also be the lighter for the Simulation part, as we need only simulate values for the first two principal components. Still, we have to be aware that the performance is worse than we initially expected.

### Operational Validation
As part of our final validation processes, we conducted a test to ensure that our model is actually useful to predict the Answer variable.
To do that, we separated the dataset into a train sample with $67\%$ of the entries and trained a new pca_model. We then used that model on the remaining $33\%$ of the data (test data) and computed the squared root of the mean squared error between the true values for the answer variable.

```{r}
n <- nrow(data)
data.sample1 <- sample(1:n, round(0.67*n))
train_data <- data[data.sample1, ] 
test_data <- data[-data.sample1, ] 

train_pca<-prcomp(train_data[, -ncol(train_data)])
train_pca_data <- as.data.frame(cbind(pca$x[,1:2], train_data$Answer))
train_pca_model <- lm(V3 ~ ., data = train_pca_data)

factors2pca <- function(row){
      return(train_pca$rotation %*% row)
}
test_pca_data <- apply(test_data[-11], 1, factors2pca)[1:2,]
test_pca_data <- as.data.frame(t(test_pca_data))
names(test_pca_data) <- c("PC1", "PC2")
yhat<-predict(train_pca_model, test_pca_data, interval="prediction")

### Root Mean Square Error ###

test_RMSE<-sqrt(mean((test_data$Answer-yhat[,1])^2)/(nrow(test_data)))
test_RMSE

```
The code above is not deterministic, as every time it collects a different randomized sample from the data. In all the trials we did, we never got an error above $0.7$ which is fairly good.

### Queue Simulation Model
For this part we ran a queuing simulation model on GPSS. Looking back at the linear expression for our PCA model: 
$$
 Y = \beta_0 + \beta_1 * PC_1 + \beta_2 * PC_2
$$
The intercept $\beta_0\simeq 44$ is to be interpreted as the arrival time, the principal components times their coefficients, $Q_1 = \beta_1 * PC_1 $ and $Q_2 =  \beta_2 * PC_2$, are to be interpreted as two succeeding queuing times, and the Answer variable predictor $Y$ is the final exit time.

The first thing we need to do is decide how to parametrize the queuing times by inspecting the data of our model:
```{r}
queue_time1 <- pca_model$coefficients[2]*pca_data$PC1
queue_time2 <- pca_model$coefficients[3]*pca_data$PC2
```

```{r, echo=FALSE}
hist(queue_time1)
print(paste("Queue 1: mean=",mean(queue_time1)," sd=", sd(queue_time1)))

hist(queue_time2)
print(paste("Queue 2: mean=",mean(queue_time2)," sd=", sd(queue_time2)))
```
We decided to paremetrize them as normal distributions:
$$
Q_1 \sim N(0, 4) \quad Q_2 \sim N(0, 17)
$$

However, these $Q_1, Q_2$ can take negative values, which we feared may raise an exception in GPSS as there is no clear interpretation of what "negative time" is. 

To prevent this issue, we inspected for a lower bound for $Q_1, Q_2$
```{r}
c(min(queue_time1),min(queue_time2))
```
And took $-20$ for $Q_1$ and $-50$ for $Q_2$. Indeed, the probabilities of this random variables reaching a value inferior to these are almost $0$ :

```{r, echo=FALSE}
print(paste("prob(Q_1 < -10) =", pnorm(-20, sd=4), ", prob(Q_2 < -40) = ", pnorm(-80, sd=17)))
```

The code for our GPSS model is:
```{r, eval=FALSE}
	GENERATE	44,1,0,1,1	
	QUEUE	Arrival
	SEIZE 	Qtime1
	DEPART 	Arrival
	ADVANCE	(20+NORMAL(1,0,4))		
	RELEASE 	Qtime1
	SEIZE 	Qtime2
	ADVANCE	(80+NORMAL(1,0,17))
	RELEASE	Qtime2
	TERMINATE	1

	START 1
```
Which simulates a queuing process for a single entity, with a termination time:
$$
 T = 44 + 20 + Q_1 + 80 + Q_2
$$
If we then subtract the values for the lower bounds $20 + 80 = 100$ we can get an approximation for the Answer variable.
$$
Answer \simeq T -100
$$

We did not extend the GPPS model to process several entities, as we did not use it any further.



## 3. Design of Experiments

We conducted a $2k$ factorial design using the minimum and maximum values for each variable to detect the effects and the interactions of the $10$ factors.The design is a combination of all possible values for each factor and find the best value, which is the minimal value in our experiment.
And we used Yates' algorithm for randomizing the order of runs in the experimental design. For instance , A value with a single "+" in the "$Factor1$" column is representing the principal effect of "$Factor1$". And A row with two "+" on "Factor1" and "$Factor2$" corresponds to the interaction of "$Factor1 * Factor2$", etc.

```{r}
data_min <- apply(data, 2, min)
data_max <- apply(data, 2, max)
  
  factorial_design <- expand.grid(Factor1 = c(data_min[1], data_max[1]),
                                  Factor2 = c(data_min[2], data_max[2]),
                                  Factor3 = c(data_min[3], data_max[3]),
                                  Factor4 = c(data_min[4], data_max[4]),
                                  Factor5 = c(data_min[5], data_max[5]),
                                  Factor6 = c(data_min[6], data_max[6]),
                                  Factor7 = c(data_min[7], data_max[7]),
                                  Factor8 = c(data_min[8], data_max[8]),
                                  Factor9 = c(data_min[9], data_max[9]),
                                  Factor10 = c(data_min[10], data_max[10]))
  get_answer <- function(row){
      pca_factors <- pca$rotation %*% unlist(row)
      answer = pca_model$coefficients %*% c(1,pca_factors[c(1,2)]) 
      return(answer)
  }
  predictions <- apply(factorial_design, 1, get_answer)
  mp <- c('-', '+')
  fnames <- list(Factor1 = mp, Factor2 = mp , Factor3 = mp, Factor4 = mp, Factor5 = mp,
                 Factor6 = mp, Factor7 = mp , Factor8 = mp, Factor9 = mp, Factor10 = mp)
  library(dae)
  treats <-fac.gen(generate = fnames,order='yates')
  y <- apply(factorial_design, 1, get_answer)
  
  
  treats <- data.frame(treats)
  treats$Y <- c(y)
  treats[1:10,]
```

From the result ,we got the most effective Factor is "$Factor 6$"($20.61508$) & "$Factor 7$"($-13.72242$) & "$Factor8$"($-11.57796$) & "$Factor9$"($-14.95543$) & "$Factor10$"($-13.54805$), besides, there is no interaction between each factor.

We assume the reason why there are no interaction between among these 10 factors is because the characteristics of the data used for the analysis can influence the presence or absence of interaction. Such that the factors do not exhibit interaction,so the analysis would reflect that.

```{r}
aov.res <-aov(y~Factor1*Factor2*Factor3*Factor4*Factor5*Factor6*Factor7*Factor8*Factor9*Factor10,treats)
  yates<- yates.effects(aov.res, data = treats)
  # Only print the effects that are not so close to zero
  for (i in 1:length(yates)){
    if (abs(yates[i]) > 0.1) print(yates[i])
  }
```
## 4. Validation
