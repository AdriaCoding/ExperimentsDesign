---
title: "SMDE: Second Assignment"
author: "Adri√† Lisa"
date: "2023-12-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## 1. Defining our own RNG
We define a function to define $n$ pseudo-random numbers between $[0,1)$ with a given *seed*, for reproducibility.
A congruential generator was used, with the following hyperparameters:

* Module $m = 2^32$.
* Multiplicative factor $a = 4 \cdot 777 + 1$.
* Additive factor $c = 50033$.

A list of RN is generated via the recurrence $Z_i = a Z_{i-1} + c \,\, (mod \,m)$, where $Z_0$ is the value of the *seed*, set by default to the prime number $26107$.
```{r}
My_RNG <- function (n, seed=26107) {
  #Congruent mixt sequence of numbers (best if c and Z0 are prime)
  m = 2^32; a = 4*777+1; c = 50033 ;
  R = integer(n); R[1] <- (a*seed + c)%%m 
  for (i in 1:(n-1)){
    R[i+1] <- (a*R[i] + c)%%m
  }
  R = R / m # in [0, 1)
  return(R)
}
```
Our teacher (Pau Fonseca) explained in class that although it is not mathematically proven, it has been empirically observed that choosing the seed to be a prime number often leads to better results.

As part of our validation process, we tested our RNG with different empirical tests from the ```{r} randtests``` library, obtaining a reasonably high confidence in the hypothesis of "randomness".

```{r}
R = My_RNG(10000)
library(randtests)
bartels.rank.test(R)
turning.point.test(R)
cox.stuart.test(R)
difference.sign.test(R)
rank.test(R)
```
## 2. Simulating our data
We created a table of numeric data, using R's standard RNG, and save it in the file "rngdata.csv".

The first 5 factors in our dataset are just random numbers with a uniform distribution, $F_1,F_2, F_3, F_4, F_5 \sim U(0,10)$.

```{r}
# Set seed for reproducibility
set.seed(123)

# Number of individuals and factors
num_individuals <- 2000
num_factors <- 10

# Create an empty data frame
data <- data.frame(matrix(NA, nrow = num_individuals, ncol = num_factors + 1))
colnames(data) <- c(paste("Factor", 1:num_factors, sep = ""), "Answer")

# Define distributions for Factor 1 to 5
for (i in 1:5) {
  data[, i] <- runif(num_individuals, min=0, max = 10)
}

```
The last 5 factors are linear combinations of the previous ones, namely:
 * $F_6 = 6F_1 + F_3$
 * $F_7 = F_1 + F_2 + F_3 + F_4 + F_5$
 * $F_8 = F_1 - F_5$
 * $F_9 = 3F_3 - 2 F_4$
 * $F_{10} = -7F_2 - 5 F_4$
```{r}

# Define linear functions for Factor 6 to 10

data$Factor6 <- 6*data$Factor1 + data$Factor3
data$Factor7 <- data$Factor1 + data$Factor2 + data$Factor3 + data$Factor4 + data$Factor5
data$Factor8 <- data$Factor1 - data$Factor5
data$Factor9 <- 3*data$Factor3 - 2*data$Factor4
data$Factor10 <- -7*data$Factor2 - 5*data$Factor4
```
Finally, an Answer variable is defined as the sum of factors $F_4, F_5, F_6$ + a random noise $Z \sim N(0,1)$.
```{r}
# Answer variable is just the sum of all factors + noise
data$Answer <- rowSums(data[,4:6]) + rnorm(num_individuals)
write.csv(data, "rngdata.csv", row.names = FALSE)
```
With this definitions, one can see that in actuality, we have a formula for the Answer variable using only independent Factors:
$$
Answer = 6*F_1 + F_3 + F_4 + F_5 + Z
$$
And by the fact that $F2$ does not appear in this formula, we can expect to find that it will not be a useful factor when we try to predict the Answer variable.


## 3. Obtain an expression to generate new data
In this section we forget our knowledge about the dataset, and wish to develop a linear model for the Answer variable.

#### Correlation Analysis
First, we will look at the correlation between the Answer variable and our Factors.
```{r}
library(tidyverse)

# Read data from a CSV file, in case we want to use the code for other datasets. 
data <- read.csv("rngdata.csv")

# Correlation matrix
cor_matrix <- cor(data)
print(cor_matrix[,11])

```
Factors $F1, F6, F7, F8$ are the most correlated with $Answer$.

#### SLR Analysis
Then, we explored simple linear regression (SLR) models for every Factor:
$$
 Y = \beta_0 + \beta_1 * F_i \quad \quad \forall i\in[1, dots,  10] 
$$
From all the information we can get from a linear model, we are only interested in two things:
 * The $R^2$ values, which account for the ability of the linear model to explain the variability of the data.
 * The $p$-value of the significance for $\beta_i$, that is, $Prob(\beta_1 \neq 0)$.


```{r}
lm_list <- vector("list", length = 10)

# Simple linear regression for each factor vs. Answer
for (i in 1:10) {
  lm_list[[i]]<- lm(Answer ~ data[, i], data = data)
  model_summary <- summary(lm_list[[i]])
  print(paste("Factor", i, "--> R^2 = ", model_summary$r.squared, 
              ", t test p-value =", model_summary$coefficients[2,4]))
}
```
Factors $F1, F6, F7, F8$ are again shown to be the most relevant, with factor $F6$ having a whopping $R^2 = 0.94$ in its simple LRM. 
Notably, the SLR models of Factors $F2, F9$ are very poor. For instance, we can not accept that the coefficent for $F9$ is signicant at all, as the $p-value$ is close to $0.2$.

#### MLR model
We now see what happens when we build a multiple linear regression (MLR), with all the factors:
$$
Y = \beta_0 + \sum_{i=1}^{10} \beta_i * F_i 
$$
```{r}
# Initial Multiple linear regression model
first_mlr_model <- lm(Answer ~ ., data = data)
summary(first_mlr_model)
```
Factors $F6, \dots, F10$ are automatically discarded. This is not surprising as we now that they are actually linearly dependant on the first Factors $F1, \dots, F5$. 

We can see that factor $F2$ has a very small and insignificant coefficient $\beta_2 \simeq 0$, so we may just as well exclude it from the model.

To be completely sure about the correctness of removing $F2$, we can also conduct a partial F-test:

```{r}
# Final Multiple linear regression model
mlr_model <- lm(Answer ~ Factor1 + Factor3 + Factor4 + Factor5, data = data)

# Partial F-Test
SSE_reduced_model <- sum((mlr_model$residuals)^2)
SSE_full_model <- sum((first_mlr_model$residuals)^2)
removed_factors <- mlr_model$df.residual - first_mlr_model$df.residual # 1
F_statistic <- (SSE_reduced_model - SSE_full_model)/removed_factors
F_statistic <- F_statistic * first_mlr_model$df.residual/SSE_reduced_model
p_value <- pf(F_statistic, removed_factors, first_mlr_model$df.residual)
if (p_value < 0.05) {
  print("We can safely remove Factor2 from the model!")
} else {
  print("Factor2 should not be removed from the model")
}
```
The final MLR model has this shape:

$$
Y = \beta_0 + \beta_1 * F_1 + \beta_2 * F_3 + \beta_3 * F_4 + \beta_4 * F_5
$$

We shall now test the assumptions of this MLR model, as part of our validation processes (see part 4 of the assignment).
###### 1-Linearity of the relationship in the data 
```{r}
# We expect to see an red line close to the horizontal at 0.
plot(mlr_model, 1)
```
###### 2- Residuals Normally distributed
```{r}
# We expect the QQ plot to follow the straight dashed line
plot(mlr_model, 2)
```

```{r}
# Shapiro-Wilks Test, we expect p-value > 0.1 (see R documentation)
shapiro.test(residuals(mlr_model))
```
###### 3-Homoscedasticity

```{r}
# We want all the residuals to have the same finite variance.
library(lmtest)
bptest(mlr_model) # We expect p-value > 0.05 (most common significance value)
```
###### 4-Independence of the errors
```{r}
# Durbin-Watson test for autocorrelation in the errors.
# We expect p-value > 0.05 to accept the independence hypothesis.
dwtest(mlr_model)
```

###### 5-Factors are uncorrelated
```{r}
# Check the Variance Inflation Factors (VIF)
# We expect all of them to be close to one.
library(car)
vif(mlr_model)
```

All the tests pass, so we conclude our MLR model is valid.

#### PCA model

However, we believe that we can build a lighter model, that contains less than 4 input variables.
To do so, we now perform a principal component analysis (PCA).

```{r}
pca<-prcomp(data[, -ncol(data)])
summary(pca)
```

Since we can achive 88% explainablity for the variance by just taking the first 2 components, we will build a linear model from PC1 and PC2. 
```{r}
pca_data <- as.data.frame(cbind(pca$x[,1:2], data$Answer))
pca_model <- lm(V3 ~ ., data = pca_data)
summary(pca_model)

```
All the test statistics of this model are also very good, so we will prefer this one over the MLR with all the factors because it is lighter. 

Even so, we will now make some tests on this model

##### 1-Linearity of the relationship in the data 
```{r}
# We expect to see an red line close to the horizontal at 0.
plot(pca_model, 1)
```
###### 2- Residuals Normally distributed
```{r}
# We expect the QQ plot to follow the straight dashed line
plot(pca_model)
```

```{r}
# Shapiro-Wilks Test, we expect p-value > 0.1 (see R documentation)
shapiro.test(residuals(pca_model))
```
###### 3-Homoscedasticity

```{r}
# We want all the residuals to have the same finite variance.
library(lmtest)
bptest(pca_model) # We expect p-value > 0.05 (most common significance value)
```
###### 4-Independence of the errors
```{r}
# Durbin-Watson test for autocorrelation in the errors.
# We expect p-value > 0.05 to accept the independence hypothesis.
dwtest(pca_model)
```

###### 5-Factors are uncorrelated
```{r}
# Check the Variance Inflation Factors (VIF)
#
# This holds by definition in models from a PCA,
# so all of them will be exactly 1.
library(car)
vif(pca_model)
```



```{r}

queue_time1 <- pca_model$coefficients[2]*pca_data$PC1
mean(queue_time1)
min(queue_time1)
sd(queue_time1)
pca_model$coefficients[3]
queue_time2 <- pca_model$coefficients[3]*pca_data$PC2
mean(queue_time2)
min(queue_time2)
sd(queue_time2)
```

```{r}
hist(data$Answer)
```
```{r}
###Operational Validation###
n <- nrow(pca_data)
data.sample1 <- sample(1:n, round(0.67*n))
data.set1 <- pca_data[data.sample1, ] 
test.set1 <- pca_data[-data.sample1, ] 
data.model1 <- pca_model

yhat<-predict(data.model1, data.set1, interval="prediction")
yhat

y<-test.set1$Answer

error<-cbind(yhat[,1,drop=FALSE],y,(y-yhat[,1])^2)
sqr_err<-error[,1]
mse<-mean(sqr_err)

### Root Mean Square Error ###
RMSE1<-sqrt(mse/(nrow(test.set1)))
RMSE1

names(data.model1)
RMSE_data1<- sqrt(mean((pca_model$residuals)^2)/nrow(data.set1))
RMSE_data1
```

3.Define a DOE(Definition of Experiments) to explore with the parametrization of the 10 factors the answer obtains the best value (we consider the minimum one to be the best value).

```{r}
exp <- data$Answer-1
model <- aov(data$Answer~exp)
summary.aov(model)

```

```{r}
eval_with_pca <- function(factors, rotation, pca_model) {
  pca_factors <- rotation %*% unlist(factors)
  answer = pca_model$coefficients %*% c(1,pca_factors[c(1,2)]) 
  return(answer)
}

neutral <- function(data){
  sdevs <- apply(data, 2, sd)
  means <- apply(data ,2, mean)
  rand <-rnorm(10,means, sdevs)
  return(rand)
}

run_experiments <- function(){
  
  data_min <- apply(data, 2, min)
  data_max <- apply(data, 2, max)
  doe <-(data[1,])
  doe[1,1:10] <- neutral(data)
  doe <- cbind(Level="neutral", doe)
  
  doe[1, ]$Answer <- eval_with_pca(doe[1,-c(1,12)], pca$rotation, pca_model)
  
  for (i in 1:10) {
    newdata<-neutral(data)
    names(newdata)<-NULL
    newdata[i]<-data_min[i]
    ans <- eval_with_pca(newdata, pca$rotation, pca_model)
    row<-c(paste("-", i), newdata, ans)
    doe[2*i,]<-row
    newdata[i]<-data_max[i]
    ans <- eval_with_pca(newdata, pca$rotation, pca_model)
    row<-c(paste("+", i), newdata, ans)
    doe[2*i+1,]<-row
  
  }
  for(j in 1:9){
    for( k in (j+1):10){
    newdata<-neutral(data)
    newdata[j]<-data_min[j]
    newdata[k]<-data_min[k]
    ans <- eval_with_pca(newdata, pca$rotation, pca_model)
    row<-c(paste("-_-", j,k), newdata, ans)
    doe <- rbind(doe,row)
    newdata[j]<-data_min[j]
    newdata[k]<-data_max[k]
    ans <- eval_with_pca(newdata, pca$rotation, pca_model)
    row<-c(paste("-_+", j,k), newdata, ans)
    doe <- rbind(doe,row)
    newdata[j]<-data_max[j]
    newdata[k]<-data_min[k]
    ans <- eval_with_pca(newdata, pca$rotation, pca_model)
    row<-c(paste("+_-", j,k), newdata, ans)
    doe <- rbind(doe,row)
    newdata[j]<-data_max[j]
    newdata[k]<-data_max[k]
    ans <- eval_with_pca(newdata, pca$rotation, pca_model)
    row<-c(paste("+_+", j,k), newdata, ans)
    doe <- rbind(doe,row)
    }
  }
  return(doe[c(1,12)])
}
doe <- run_experiments()
```

ANOVA tests for 1-factor and 2-factor

```{r, eval=FALSE}

subset_data <- doe[2:201, ]
# 1-factor ANOVA
for (i in 1:10) {
  subset_data[, i] <- as.factor(subset_data[, i])
  doe_model <- aov(Answer ~ subset_data[, i+1], data = subset_data)
  print(paste("One-Factor ANOVA for Factor", i, "vs. Answer:"))
  print(summary(doe_model))
}

```
```{r, eval=FALSE}
subset_data2 <- doe_model[2:144, ] 

for (i in 1:9) {
  for (j in (i + 1):10) {
    subset_data2[, 1:10] <- lapply(subset_data2[, 1:10], as.factor)
    # Two-Factor ANOVA
    doe_model <- aov(Answer ~ subset_data2[, i+1] + subset_data2[, j+1], data = subset_data2)
    # Print ANOVA summary
    print(paste("Two-Factor ANOVA for Factor", i, "and Factor", j, "vs. Answer:"))
    print(summary(doe_model))
  }
}

```
